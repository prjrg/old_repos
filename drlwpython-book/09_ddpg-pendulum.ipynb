{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v0\").unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_shape = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_shape = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_bound = [env.action_space.low, env.action_space.high]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "tau = 0.001\n",
    "replay_buffer = 10000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    def __init__(self, state_shape, action_shape, high_action_value):\n",
    "        self.replay_buffer = np.zeros((replay_buffer, state_shape * 2 + action_shape + 1), dtype=np.float32)\n",
    "        self.num_transitions = 0\n",
    "        self.sess = tf.Session()\n",
    "        self.noise = 3.0\n",
    "        self.state_shape, self.action_shape, self.high_action_value = state_shape, action_shape, high_action_value\n",
    "        self.state = tf.placeholder(tf.float32, [None, state_shape], 'state')\n",
    "        self.next_state = tf.placeholder(tf.float32, [None, state_shape], 'next_state')\n",
    "        self.reward = tf.placeholder(tf.float32, [None, 1], 'reward')\n",
    "        \n",
    "        with tf.variable_scope('Actor'):\n",
    "            self.actor = self.build_actor_network(self.state, scope='main', trainable=True)\n",
    "            self.target_actor = self.build_actor_network(self.next_state, scope='target', trainable=False)\n",
    "        with tf.variable_scope('Critic'):\n",
    "            self.critic = self.build_critic_network(self.state, self.actor, scope='main', trainable=True)\n",
    "            self.target_critic = self.build_critic_network(self.next_state, self.target_actor, scope='target', trainable=False)\n",
    "            \n",
    "        self.main_actor_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/main')\n",
    "        self.target_actor_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target')\n",
    "        self.main_critic_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/main')\n",
    "        self.target_critic_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')\n",
    "        \n",
    "        self.soft_replacement = [[\n",
    "            tf.assign(phi_, tau*phi + (1-tau)*phi_), tf.assign(theta_, tau*theta + (1-tau)*theta_)\n",
    "        ]for phi, phi_, theta, theta_ in zip(self.main_actor_params, self.target_actor_params, self.main_critic_params, self.target_critic_params)]\n",
    "        \n",
    "        y = self.reward + gamma * self.target_critic\n",
    "        mse_loss = tf.losses.mean_squared_error(labels=y, predictions=self.critic)\n",
    "        self.train_critic = tf.train.AdamOptimizer(0.01).minimize(mse_loss, name=\"adam-ink\", var_list=self.main_critic_params)\n",
    "        actor_loss = -tf.reduce_mean(self.critic)\n",
    "        self.train_actor = tf.train.AdamOptimizer(0.001).minimize(actor_loss, var_list=self.main_actor_params)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        action = self.sess.run(self.actor, {self.state: state[np.newaxis, :]})[0]\n",
    "        action = np.random.normal(action, self.noise)\n",
    "        \n",
    "        action = np.clip(action, action_bound[0], action_bound[1])\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def train(self):\n",
    "        self.sess.run(self.soft_replacement)\n",
    "        indices = np.random.choise(replay_buffer, size=batch_size)\n",
    "        \n",
    "        batch_transition = self.replay_buffer[indices, :]\n",
    "        \n",
    "        batch_states = batch_transition[:, :self.state_shape]\n",
    "        batch_actions = batch_transition[:, self.state_shape: self.state_shape + self.action_shape]\n",
    "        batch_rewards = batch_transition[:, -self.statep_shape-1:-self.state_shape]\n",
    "        batch_next_state = batch_transition[:, -self.state_shape:]\n",
    "        \n",
    "        self.sess.run(self.train_actor, {self.state: batch_states})\n",
    "        self.sess.run(self.train_critic, {self.state: batch_states, self.actor: batch_actions, self.reward: batch_rewards, self.next_state: batch_next_state})\n",
    "        \n",
    "    def store_transition(self, state, actor, reward, next_state):\n",
    "        trans = np.hstack((state, actor, [reward], next_state))\n",
    "        \n",
    "        index = self.num_transitions % replay_buffer\n",
    "        self.replay_buffer[index, :] = trans\n",
    "        self.num_transitions += 1\n",
    "        \n",
    "        if self.num_transitions > replay_buffer:\n",
    "            self.noise *= 0.99995\n",
    "            self.train()\n",
    "            \n",
    "    def build_actor_network(self, state, scope, trainable):\n",
    "        with tf.variable_scope(scope):\n",
    "            layer_1 = tf.layers.dense(state, 30, activation=tf.nn.tanh, name='layer_1', trainable = trainable)\n",
    "            actor = tf.layers.dense(layer_1, self.action_shape, activation = tf.nn.tanh, name='actor', trainable=trainable)\n",
    "            return tf.multiply(actor, self.high_action_value, name=\"scaled_a\")\n",
    "    \n",
    "    def build_critic_network(self, state, actor, scope, trainable):\n",
    "        with tf.variable_scope(scope):\n",
    "            w1_s = tf.get_variable('w1_s', [self.state_shape, 30], trainable=trainable)\n",
    "            w1_a = tf.get_variable('w1_a', [self.action_shape, 30], trainable=trainable)\n",
    "            b1 = tf.get_variable('b1', [1, 30], trainable=trainable)\n",
    "            net = tf.nn.tanh(tf.matmul(state, w1_s) + tf.matmul(actor, w1_a) + b1)\n",
    "            critic = tf.layers.dense(net, 1, trainable=trainable)\n",
    "            return critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg = DDPG(state_shape, action_shape, action_bound[1])\n",
    "num_episodes = 300\n",
    "num_timesteps = 500\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    Return = 0\n",
    "    for t in range(num_timesteps):\n",
    "        env.render()\n",
    "        action = ddpg.select_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        ddpg.store_transition(state, action, reward, next_state)\n",
    "        \n",
    "        Return += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        state = next_state\n",
    "        \n",
    "    if i %10 ==0:\n",
    "        print(\"Episode:{}, Return: {}\".format(i,Return))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
