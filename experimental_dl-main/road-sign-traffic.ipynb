{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-14 11:48:10--  https://raw.githubusercontent.com/georgesung/traffic_sign_classification_german/master/signnames.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.132.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.132.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 999 [text/plain]\n",
      "Saving to: ‘signnames.csv’\n",
      "\n",
      "signnames.csv       100%[===================>]     999  --.-KB/s    in 0s      \n",
      "\n",
      "2020-12-14 11:48:11 (102 MB/s) - ‘signnames.csv’ saved [999/999]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.exists('GTSRB'):\n",
    "    !pip install -U -q torch_snippets\n",
    "    !wget -qq https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Training_Images.zip\n",
    "    !wget -qq https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_Images.zip\n",
    "    !unzip -qq GTSRB_Final_Training_Images.zip\n",
    "    !unzip -qq GTSRB_Final_Test_Images.zip\n",
    "    !wget https://raw.githubusercontent.com/georgesung/traffic_sign_classification_german/master/signnames.csv\n",
    "    !rm GTSRB_Final_Training_Images.zip GTSRB_Final_Test_Images.zip\n",
    "\n",
    "from torch_snippets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classIds = pd.read_csv('signnames.csv')\n",
    "classIds.set_index('ClassId', inplace=True)\n",
    "classIds = classIds.to_dict()['SignName']\n",
    "classIds = {f'{k:05d}':v for k,v in classIds.items()}\n",
    "id2int = {v:ix for ix,(k,v) in enumerate(classIds.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as T\n",
    "trn_tfms = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "                T.Resize(32),\n",
    "                T.CenterCrop(32),\n",
    "                T.ColorJitter(brightness=(0.8,1.2), \n",
    "                contrast=(0.8,1.2), \n",
    "                saturation=(0.8,1.2), \n",
    "                hue=0.25),\n",
    "                T.RandomAffine(5, translate=(0.01,0.1)),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                            std=[0.229, 0.224, 0.225]),\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tfms = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "                T.Resize(32),\n",
    "                T.CenterCrop(32),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                            std=[0.229, 0.224, 0.225]),\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTSRB(Dataset):\n",
    "\n",
    "    def __init__(self, files, transform=None):\n",
    "        self.files = files\n",
    "        self.transform = transform\n",
    "        logger.info(len(self))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        fpath = self.files[ix]\n",
    "        clss = fname(parent(fpath))\n",
    "        img = read(fpath, 1)\n",
    "        return img, classIds[clss]\n",
    "\n",
    "    def choose(self):\n",
    "        return self[randint(len(self))]\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        imgs, classes = list(zip(*batch))\n",
    "        if self.transform:\n",
    "            imgs =[self.transform(img)[None] \\\n",
    "                   for img in imgs]\n",
    "        classes = [torch.tensor([id2int[clss]]) \\\n",
    "                   for clss in classes]\n",
    "        imgs, classes = [torch.cat(i).to(device) \\\n",
    "                         for i in [imgs, classes]]\n",
    "        return imgs, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-14 11:50:44.934 | INFO     | torch_snippets.loader:Glob:190 - 39209 files found at GTSRB/Final_Training/Images/*/*.ppm\n",
      "2020-12-14 11:50:45.103 | INFO     | __main__:__init__:6 - 29406\n",
      "2020-12-14 11:50:45.104 | INFO     | __main__:__init__:6 - 9803\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "all_files = Glob('GTSRB/Final_Training/Images/*/*.ppm')\n",
    "np.random.seed(10)\n",
    "np.random.shuffle(all_files)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "trn_files, val_files = train_test_split(all_files, \\\n",
    "                                        random_state=1)\n",
    "\n",
    "trn_ds = GTSRB(trn_files, transform=trn_tfms)\n",
    "val_ds = GTSRB(val_files, transform=val_tfms)\n",
    "trn_dl = DataLoader(trn_ds, 32, shuffle=True, \\\n",
    "                    collate_fn=trn_ds.collate_fn)\n",
    "val_dl = DataLoader(val_ds, 32, shuffle=False, \\\n",
    "                    collate_fn=val_ds.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "def convBlock(ni, no):\n",
    "    return nn.Sequential(\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Conv2d(ni, no, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm2d(no),\n",
    "                nn.MaxPool2d(2),\n",
    "            )\n",
    "    \n",
    "class SignClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "                        convBlock(3, 64),\n",
    "                        convBlock(64, 64),\n",
    "                        convBlock(64, 128),\n",
    "                        convBlock(128, 64),\n",
    "                        nn.Flatten(),\n",
    "                        nn.Linear(256, 256),\n",
    "                        nn.Dropout(0.2),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        nn.Linear(256, len(id2int))\n",
    "                    )\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def compute_metrics(self, preds, targets):\n",
    "        ce_loss = self.loss_fn(preds, targets)\n",
    "        acc =(torch.max(preds, 1)[1]==targets).float().mean()\n",
    "        return ce_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model, data, optimizer, criterion):\n",
    "    model.train()\n",
    "    ims, labels = data\n",
    "    _preds = model(ims)\n",
    "    optimizer.zero_grad()\n",
    "    loss, acc = criterion(_preds, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), acc.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_batch(model, data, criterion):\n",
    "    model.eval()\n",
    "    ims, labels = data\n",
    "    _preds = model(ims)\n",
    "    loss, acc = criterion(_preds, labels)\n",
    "    return loss.item(), acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1.000\ttrn_loss: 1.877\ttrn_acc: 0.449\tval_loss: 0.809\tval_acc: 0.761\t(24.65s - 1207.95s remaining)\n",
      "EPOCH: 2.000\ttrn_loss: 0.690\ttrn_acc: 0.778\tval_loss: 0.832\tval_acc: 0.761\t(49.04s - 1176.99s remaining)\n",
      "EPOCH: 3.000\ttrn_loss: 0.479\ttrn_acc: 0.843\tval_loss: 0.675\tval_acc: 0.804\t(73.33s - 1148.91s remaining)\n",
      "EPOCH: 4.000\ttrn_loss: 0.391\ttrn_acc: 0.872\tval_loss: 0.582\tval_acc: 0.844\t(97.38s - 1119.88s remaining)\n",
      "EPOCH: 5.000\ttrn_loss: 0.326\ttrn_acc: 0.893\tval_loss: 0.423\tval_acc: 0.881\t(121.32s - 1091.92s remaining)\n",
      "EPOCH: 6.000\ttrn_loss: 0.290\ttrn_acc: 0.906\tval_loss: 0.523\tval_acc: 0.867\t(145.23s - 1065.00s remaining)\n",
      "EPOCH: 7.000\ttrn_loss: 0.272\ttrn_acc: 0.912\tval_loss: 0.330\tval_acc: 0.893\t(169.01s - 1038.20s remaining)\n",
      "EPOCH: 8.000\ttrn_loss: 0.239\ttrn_acc: 0.921\tval_loss: 0.357\tval_acc: 0.895\t(192.86s - 1012.51s remaining)\n",
      "EPOCH: 9.000\ttrn_loss: 0.222\ttrn_acc: 0.927\tval_loss: 0.525\tval_acc: 0.865\t(216.60s - 986.74s remaining)\n",
      "EPOCH: 10.000\ttrn_loss: 0.205\ttrn_acc: 0.933\tval_loss: 0.335\tval_acc: 0.907\t(239.93s - 959.71s remaining)\n",
      "EPOCH: 11.000\ttrn_loss: 0.198\ttrn_acc: 0.934\tval_loss: 0.366\tval_acc: 0.898\t(263.76s - 935.16s remaining)\n",
      "EPOCH: 12.000\ttrn_loss: 0.144\ttrn_acc: 0.952\tval_loss: 0.295\tval_acc: 0.921\t(287.76s - 911.25s remaining)\n",
      "EPOCH: 13.000\ttrn_loss: 0.124\ttrn_acc: 0.958\tval_loss: 0.263\tval_acc: 0.928\t(311.75s - 887.30s remaining)\n",
      "EPOCH: 14.000\ttrn_loss: 0.117\ttrn_acc: 0.959\tval_loss: 0.249\tval_acc: 0.931\t(335.80s - 863.49s remaining)\n",
      "EPOCH: 15.000\ttrn_loss: 0.113\ttrn_acc: 0.963\tval_loss: 0.261\tval_acc: 0.929\t(359.70s - 839.29s remaining)\n",
      "EPOCH: 16.000\ttrn_loss: 0.107\ttrn_acc: 0.964\tval_loss: 0.275\tval_acc: 0.926\t(383.69s - 815.34s remaining)\n",
      "EPOCH: 17.000\ttrn_loss: 0.100\ttrn_acc: 0.966\tval_loss: 0.290\tval_acc: 0.927\t(407.71s - 791.43s remaining)\n",
      "EPOCH: 18.000\ttrn_loss: 0.100\ttrn_acc: 0.966\tval_loss: 0.237\tval_acc: 0.934\t(431.76s - 767.57s remaining)\n",
      "EPOCH: 19.000\ttrn_loss: 0.095\ttrn_acc: 0.968\tval_loss: 0.300\tval_acc: 0.921\t(456.06s - 744.10s remaining)\n",
      "EPOCH: 20.000\ttrn_loss: 0.095\ttrn_acc: 0.968\tval_loss: 0.292\tval_acc: 0.923\t(480.10s - 720.15s remaining)\n",
      "EPOCH: 21.000\ttrn_loss: 0.088\ttrn_acc: 0.969\tval_loss: 0.212\tval_acc: 0.943\t(504.14s - 696.19s remaining)\n",
      "EPOCH: 22.000\ttrn_loss: 0.090\ttrn_acc: 0.969\tval_loss: 0.293\tval_acc: 0.922\t(528.30s - 672.38s remaining)\n",
      "EPOCH: 23.000\ttrn_loss: 0.089\ttrn_acc: 0.970\tval_loss: 0.303\tval_acc: 0.925\t(552.77s - 648.91s remaining)\n",
      "EPOCH: 24.000\ttrn_loss: 0.085\ttrn_acc: 0.970\tval_loss: 0.271\tval_acc: 0.930\t(577.19s - 625.29s remaining)\n",
      "EPOCH: 25.000\ttrn_loss: 0.080\ttrn_acc: 0.973\tval_loss: 0.242\tval_acc: 0.935\t(601.24s - 601.24s remaining)\n",
      "EPOCH: 26.000\ttrn_loss: 0.079\ttrn_acc: 0.972\tval_loss: 0.299\tval_acc: 0.924\t(624.88s - 576.82s remaining)\n",
      "EPOCH: 27.000\ttrn_loss: 0.081\ttrn_acc: 0.972\tval_loss: 0.245\tval_acc: 0.935\t(649.21s - 553.03s remaining)\n",
      "EPOCH: 28.000\ttrn_loss: 0.080\ttrn_acc: 0.972\tval_loss: 0.288\tval_acc: 0.923\t(673.27s - 529.00s remaining)\n",
      "EPOCH: 29.000\ttrn_loss: 0.075\ttrn_acc: 0.976\tval_loss: 0.215\tval_acc: 0.944\t(696.82s - 504.59s remaining)\n",
      "EPOCH: 30.000\ttrn_loss: 0.076\ttrn_acc: 0.974\tval_loss: 0.233\tval_acc: 0.936\t(720.36s - 480.24s remaining)\n",
      "EPOCH: 31.000\ttrn_loss: 0.076\ttrn_acc: 0.974\tval_loss: 0.224\tval_acc: 0.938\t(743.92s - 455.95s remaining)\n",
      "EPOCH: 32.000\ttrn_loss: 0.074\ttrn_acc: 0.975\tval_loss: 0.222\tval_acc: 0.940\t(767.45s - 431.69s remaining)\n",
      "EPOCH: 33.000\ttrn_loss: 0.075\ttrn_acc: 0.974\tval_loss: 0.215\tval_acc: 0.942\t(791.12s - 407.55s remaining)\n",
      "EPOCH: 34.000\ttrn_loss: 0.072\ttrn_acc: 0.976\tval_loss: 0.196\tval_acc: 0.948\t(814.64s - 383.36s remaining)\n",
      "EPOCH: 35.000\ttrn_loss: 0.069\ttrn_acc: 0.977\tval_loss: 0.201\tval_acc: 0.947\t(838.13s - 359.20s remaining)\n",
      "EPOCH: 36.000\ttrn_loss: 0.068\ttrn_acc: 0.977\tval_loss: 0.228\tval_acc: 0.939\t(861.80s - 335.15s remaining)\n",
      "EPOCH: 37.000\ttrn_loss: 0.070\ttrn_acc: 0.976\tval_loss: 0.215\tval_acc: 0.941\t(885.57s - 311.15s remaining)\n",
      "EPOCH: 38.000\ttrn_loss: 0.065\ttrn_acc: 0.978\tval_loss: 0.193\tval_acc: 0.948\t(909.74s - 287.29s remaining)\n",
      "EPOCH: 39.000\ttrn_loss: 0.061\ttrn_acc: 0.979\tval_loss: 0.243\tval_acc: 0.935\t(933.76s - 263.37s remaining)\n",
      "EPOCH: 40.000\ttrn_loss: 0.062\ttrn_acc: 0.979\tval_loss: 0.163\tval_acc: 0.955\t(957.25s - 239.31s remaining)\n",
      "EPOCH: 41.000\ttrn_loss: 0.063\ttrn_acc: 0.978\tval_loss: 0.167\tval_acc: 0.955\t(981.52s - 215.46s remaining)\n",
      "EPOCH: 42.000\ttrn_loss: 0.061\ttrn_acc: 0.979\tval_loss: 0.239\tval_acc: 0.936\t(1005.77s - 191.58s remaining)\n",
      "EPOCH: 43.000\ttrn_loss: 0.063\ttrn_acc: 0.978\tval_loss: 0.192\tval_acc: 0.945\t(1029.92s - 167.66s remaining)\n",
      "EPOCH: 44.000\ttrn_loss: 0.063\ttrn_acc: 0.978\tval_loss: 0.273\tval_acc: 0.929\t(1054.04s - 143.73s remaining)\n",
      "EPOCH: 45.000\ttrn_loss: 0.062\ttrn_acc: 0.979\tval_loss: 0.275\tval_acc: 0.929\t(1078.23s - 119.80s remaining)\n",
      "EPOCH: 46.000\ttrn_loss: 0.062\ttrn_acc: 0.979\tval_loss: 0.232\tval_acc: 0.938\t(1102.59s - 95.88s remaining)\n",
      "EPOCH: 47.000\ttrn_loss: 0.059\ttrn_acc: 0.978\tval_loss: 0.220\tval_acc: 0.940\t(1127.07s - 71.94s remaining)\n",
      "EPOCH: 48.000\ttrn_loss: 0.060\ttrn_acc: 0.979\tval_loss: 0.178\tval_acc: 0.950\t(1151.32s - 47.97s remaining)\n",
      "EPOCH: 49.000\ttrn_loss: 0.056\ttrn_acc: 0.981\tval_loss: 0.199\tval_acc: 0.946\t(1175.78s - 24.00s remaining)\n",
      "EPOCH: 50.000\ttrn_loss: 0.060\ttrn_acc: 0.979\tval_loss: 0.193\tval_acc: 0.948\t(1200.11s - 0.00s remaining)\n"
     ]
    }
   ],
   "source": [
    "model = SignClassifier().to(device)\n",
    "criterion = model.compute_metrics\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "n_epochs = 50\n",
    "\n",
    "log = Report(n_epochs)\n",
    "for ex in range(n_epochs):\n",
    "    N = len(trn_dl)\n",
    "    for bx, data in enumerate(trn_dl):\n",
    "        loss, acc = train_batch(model, data, optimizer, \\\n",
    "                                    criterion)\n",
    "        log.record(ex+(bx+1)/N,trn_loss=loss, trn_acc=acc, \\\n",
    "                                     end='\\r')\n",
    "\n",
    "    N = len(val_dl)\n",
    "    for bx, data in enumerate(val_dl):\n",
    "        loss, acc = validate_batch(model, data, criterion)\n",
    "        log.record(ex+(bx+1)/N, val_loss=loss, val_acc=acc, \\\n",
    "                                    end='\\r')\n",
    "        \n",
    "    log.report_avgs(ex+1)\n",
    "    if ex == 10: optimizer = optim.Adam(model.parameters(), \\\n",
    "                                    lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
