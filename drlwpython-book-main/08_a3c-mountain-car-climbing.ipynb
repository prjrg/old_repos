{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pedro/anaconda3/envs/universe/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gym\n",
    "import multiprocessing\n",
    "import threading\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "state_shape = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_shape = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_bound = [env.action_space.low, env.action_space.high]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timesteps = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_net_scope = 'Global_Net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_global = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.90\n",
    "beta = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = 'logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic:\n",
    "    def __init__(self, scope, sess, globalAC=None):\n",
    "        self.sess = sess\n",
    "        self.actor_optimizer = tf.train.RMSPropOptimizer(0.0001, name='RMSPropA')\n",
    "        self.critic_optimizer = tf.train.RMSPropOptimizer(0.001, name='RMSPropC')\n",
    "        \n",
    "        if scope == global_net_scope:\n",
    "            with tf.variable_scope(scope):\n",
    "                self.state = tf.placeholder(tf.float32, [None, state_shape], 'state')\n",
    "                self.actor_params, self.critic_params = self.build_network(scope)[-2:]\n",
    "        else:\n",
    "            with tf.variable_scope(scope):\n",
    "                self.state = tf.placeholder(tf.float32, [None, state_shape], 'state')\n",
    "                self.action_dist = tf.placeholder(tf.float32, [None, action_shape], 'action')\n",
    "                self.target_value = tf.placeholder(tf.float32, [None, 1], 'Vtarget')\n",
    "                mean, variance, self.value, self.actor_params, self.critic_params = self.build_network(scope)\n",
    "                td_error = tf.subtract(self.target_value, self.value, name='TD_error')\n",
    "                with tf.name_scope('critic_loss'):\n",
    "                    self.critic_loss = tf.reduce_mean(tf.square(td_error))\n",
    "                    normal_dist = tf.distributions.Normal(mean, variance)\n",
    "                with tf.name_scope('actor_loss'):\n",
    "                    log_prob = normal_dist.log_prob(self.action_dist)\n",
    "                    entropy_pi = normal_dist.entropy()\n",
    "                    self.loss = log_prob * td_error + (beta * entropy_pi)\n",
    "                    self.actor_loss = tf.reduce_mean(-self.loss)\n",
    "                    \n",
    "                with tf.name_scope('select_action'):\n",
    "                    self.action = tf.clip_by_value(tf.squeeze(normal_dist.sample(1), axis=0), action_bound[0], action_bound[1])\n",
    "                \n",
    "                with tf.name_scope('local_grad'):\n",
    "                    self.actor_grads = tf.gradients(self.actor_loss, self.actor_params)\n",
    "                    self.critic_grads = tf.gradients(self.critic_loss, self.critic_params)\n",
    "                    \n",
    "                with tf.name_scope('sync'):\n",
    "                    with tf.name_scope('push'):\n",
    "                        self.update_actor_params = self.actor_optimizer.apply_gradients(zip(self.actor_grads, globalAC.actor_params))\n",
    "                        self.update_critic_params = self.critic_optimizer.apply_gradients(zip(self.critic_grads, globalAC.critic_params))\n",
    "                    with tf.name_scope('pull'):\n",
    "                        self.pull_actor_params = [l_p.assign(g_p) for l_p, g_p in zip(self.actor_params, globalAC.actor_params)]\n",
    "                        self.pull_critic_params = [l_p.assign(g_p) for l_p, g_p in zip(self.critic_params, globalAC.critic_params)]\n",
    "                        \n",
    "        \n",
    "    def build_network(self, scope):\n",
    "        w_init = tf.random_normal_initializer(0., 0.1)\n",
    "        \n",
    "        with tf.variable_scope('actor'):\n",
    "            l_a = tf.layers.dense(self.state, 200, tf.nn.relu, kernel_initializer=w_init, name='la')\n",
    "            mean = tf.layers.dense(l_a, action_shape, tf.nn.tanh, kernel_initializer=w_init, name='mean')\n",
    "            variance = tf.layers.dense(l_a, action_shape, tf.nn.softplus, kernel_initializer=w_init, name='variance')\n",
    "            \n",
    "        with tf.variable_scope('critic'):\n",
    "            l_c = tf.layers.dense(self.state, 100, tf.nn.relu, kernel_initializer=w_init, name='lc')\n",
    "            value = tf.layers.dense(l_c, 1, kernel_initializer=w_init, name='value')\n",
    "            \n",
    "        actor_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope+'/actor')\n",
    "        critic_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope+'/critic')\n",
    "        \n",
    "        return mean, variance, value, actor_params, critic_params\n",
    "    \n",
    "    def update_global(self, feed_dict):\n",
    "        self.sess.run([self.update_actor_params, self.update_critic_params], feed_dict)\n",
    "        \n",
    "    def pull_from_global(self):\n",
    "        self.sess.run([self.pull_actor_params, self.pull_critic_params])\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        state = state[np.newaxis, :]\n",
    "        \n",
    "        return self.sess.run(self.action, {self.state: state})[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker:\n",
    "    def __init__(self, name, globalAC, sess):\n",
    "        self.env = gym.make('MountainCarContinuous-v0').unwrapped\n",
    "        self.name = name\n",
    "        self.AC = ActorCritic(name, sess, globalAC)\n",
    "        self.sess = sess\n",
    "        \n",
    "    def work(self):\n",
    "        global global_rewards, global_episodes\n",
    "        total_step = 1\n",
    "        batch_states, batch_actions, batch_rewards = [], [], []\n",
    "        while not coord.should_stop() and global_episodes < num_episodes:\n",
    "            state = self.env.reset()\n",
    "            total_rewards = 0\n",
    "            for t in range(num_timesteps):\n",
    "                if self.name == 'W_0':\n",
    "                    self.env.render()\n",
    "                action = self.AC.select_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                done = True if t == num_timesteps - 1 else False\n",
    "                \n",
    "                total_rewards += reward\n",
    "                \n",
    "                batch_states.append(state)\n",
    "                batch_actions.append(action)\n",
    "                batch_rewards.append((reward + 8) / 8)\n",
    "                \n",
    "                if total_step % update_global == 0 or done:\n",
    "                    if done:\n",
    "                        v_s_ = 0\n",
    "                    else:\n",
    "                        v_s_ = self.sess.run(self.AC.value, {self.AC.state: next_state[np.newaxis, :]})[0, 0]\n",
    "                    batch_target_value = []\n",
    "                    for reward in batch_rewards[::-1]:\n",
    "                        v_s_ = reward + gamma * v_s_\n",
    "                        batch_target_value.append(v_s_)\n",
    "                    batch_target_value.reverse()\n",
    "                    batch_states, batch_actions, batch_target_value = np.vstack(batch_states), np.vstack(batch_actions), np.vstack(batch_target_value)\n",
    "                    \n",
    "                    feed_dict = {\n",
    "                        self.AC.state: batch_states,\n",
    "                        self.AC.action_dist: batch_actions,\n",
    "                        self.AC.target_value: batch_target_value\n",
    "                    }\n",
    "                    \n",
    "                    self.AC.update_global(feed_dict)\n",
    "                    batch_states, batch_actions, batch_rewards = [], [], []\n",
    "                    \n",
    "                    self.AC.pull_from_global()\n",
    "                state = next_state\n",
    "                total_step += 1\n",
    "                \n",
    "                if done:\n",
    "                    if len(global_rewards) < 5:\n",
    "                        global_rewards.append(total_rewards)\n",
    "                    else:\n",
    "                        global_rewards.append(total_rewards)\n",
    "                        global_rewards[-1] = np.mean(global_rewards[-5:])\n",
    "                        global_episodes += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_rewards = []\n",
    "global_episodes = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-132905d8fc75>:20: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /home/pedro/anaconda3/envs/universe/lib/python3.6/site-packages/tensorflow/python/ops/distributions/normal.py:160: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /home/pedro/anaconda3/envs/universe/lib/python3.6/site-packages/tensorflow/python/training/rmsprop.py:123: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    global_agent = ActorCritic(global_net_scope, sess)\n",
    "    worker_agents = []\n",
    "    for i in range(num_workers):\n",
    "        i_name = 'W_%i' % i\n",
    "        worker_agents.append(Worker(i_name, global_agent, sess))\n",
    "coord = tf.train.Coordinator()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.summary.writer.writer.FileWriter at 0x7f604244ee48>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)\n",
    "tf.summary.FileWriter(log_dir, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_threads = []\n",
    "for worker in worker_agents:\n",
    "    job = lambda: worker.work()\n",
    "    t = threading.Thread(target=job)\n",
    "    t.start()\n",
    "    worker_threads.append(t)\n",
    "coord.join(worker_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
