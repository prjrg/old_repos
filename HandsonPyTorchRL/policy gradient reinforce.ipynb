{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: box2d-py in /home/pedro/anaconda3/lib/python3.7/site-packages (2.3.8)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install box2d-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00550089,  1.4009429 ,  0.557158  , -0.44343776, -0.0063673 ,\n",
       "       -0.1262046 ,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_screen = env.render(mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ffb25a686d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAQtUlEQVR4nO3df6yeZX3H8fdnLT+cGsvv1LYO0LpBjBbSYQ1mQeaPQpYVEzWQbTSGrC7BBBOzDVwycYl/mKgY4kJWI7MsDmT+GA3ZorVA3P4QLFigtSJViT22oVn4oY0JW8t3fzzX0cdzTnt+9JyeXue8X8md576/9/U893XFxw93r3M9uVNVSJL68Tvz3QFJ0vQY3JLUGYNbkjpjcEtSZwxuSeqMwS1JnZmz4E6yPslTSfYmuXmuriNJi03mYh13kiXAj4B3ASPA94DrquoHs34xSVpk5uqO+zJgb1X9pKr+F7gH2DBH15KkRWXpHH3uCmDf0PEI8NajNU7izzclaYyqykT1uQruiS72W+GcZBOwaY6uL0kL1lwF9wiwauh4JbB/uEFVbQY2g3fckjQdczXH/T1gdZILkpwKXAtsnaNrSdKiMid33FV1OMmHgW8CS4A7q2r3XFxLkhabOVkOOO1OOFUiSeMc7Y+T/nJSkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnjuuZk0meAX4JHAEOV9XaJGcCXwHOB54BPlBVzx9fNyVJo2bjjvsdVbWmqta245uB7VW1GtjejiVJs2Qupko2AFva/hbgmjm4hiQtWscb3AV8K8mjSTa12nlVdQCgvZ57nNeQJA05rjlu4PKq2p/kXGBbkh9O9Y0t6DdN2lCS9FtSVbPzQcmtwCHgL4ErqupAkuXAQ1X1+5O8d3Y6IUkLSFVlovqMp0qSvDLJq0f3gXcDu4CtwMbWbCNw30yvIUkab8Z33EkuBL7RDpcC/1pVn0xyFnAv8DrgZ8D7q+q5ST7LO25JGuNod9yzNlVyPAxuSRpv1qdKJEnzw+CWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktSZSYM7yZ1JDibZNVQ7M8m2JE+31zNaPUluT7I3yRNJLp3LzkvSYjSVO+4vAevH1G4GtlfVamB7Owa4Cljdtk3AHbPTTUnSqEmDu6q+Azw3prwB2NL2twDXDNXvqoHvAsuSLJ+tzkqSZj7HfV5VHQBor+e2+gpg31C7kVYbJ8mmJDuS7JhhHyRpUVo6y5+XCWo1UcOq2gxsBkgyYRtJ0ngzveN+dnQKpL0ebPURYNVQu5XA/pl3T5I01kyDeyuwse1vBO4bql/fVpesA14cnVKRJM2OVB17liLJ3cAVwNnAs8DHgX8H7gVeB/wMeH9VPZckwOcZrEL5FfDBqpp0DtupEkkar6ommn6ePLhPBINbksY7WnD7y0lJ6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ2ZNLiT3JnkYJJdQ7Vbk/w8yc62XT107pYke5M8leQ9c9VxSVqspvKw4D8CDgF3VdWbWu1W4FBVfXpM24uBu4HLgNcC3wbeWFVHJrmGz5yUpDFm/MzJqvoO8NwUr7MBuKeqXqqqnwJ7GYS4JGmWHM8c94eTPNGmUs5otRXAvqE2I602TpJNSXYk2XEcfZCkRWemwX0H8HpgDXAA+EyrT3RbP+E0SFVtrqq1VbV2hn2QpEVpRsFdVc9W1ZGqehn4Ar+ZDhkBVg01XQnsP74uSpKGzSi4kywfOnwvMLriZCtwbZLTklwArAYeOb4uSpKGLZ2sQZK7gSuAs5OMAB8HrkiyhsE0yDPAhwCqaneSe4EfAIeBGydbUSJJmp5JlwOekE64HFCSxpnxckBJ0snF4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOTBrcSVYleTDJniS7k9zU6mcm2Zbk6fZ6Rqsnye1J9iZ5Ismlcz0ISVpMpnLHfRj4aFVdBKwDbkxyMXAzsL2qVgPb2zHAVQye7r4a2ATcMeu9lqRFbNLgrqoDVfVY2/8lsAdYAWwAtrRmW4Br2v4G4K4a+C6wLMnyWe+5JC1S05rjTnI+cAnwMHBeVR2AQbgD57ZmK4B9Q28babWxn7UpyY4kO6bfbUlavJZOtWGSVwFfAz5SVb9IJnxqPMBEJ2pcoWozsLl99rjzkqSJTemOO8kpDEL7y1X19VZ+dnQKpL0ebPURYNXQ21cC+2enu5KkqawqCfBFYE9VfXbo1FZgY9vfCNw3VL++rS5ZB7w4OqUiSTp+qTr2LEWStwP/BTwJvNzKH2Mwz30v8DrgZ8D7q+q5FvSfB9YDvwI+WFXHnMd2qkSSxquqCeekJw3uE8HglqTxjhbc/nJSkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnpvKw4FVJHkyyJ8nuJDe1+q1Jfp5kZ9uuHnrPLUn2JnkqyXvmcgCStNhM5WHBy4HlVfVYklcDjwLXAB8ADlXVp8e0vxi4G7gMeC3wbeCNVXXkGNfwmZOSNMaMnzlZVQeq6rG2/0tgD7DiGG/ZANxTVS9V1U+BvQxCXJI0C6Y1x53kfOAS4OFW+nCSJ5LcmeSMVlsB7Bt62wjHDnpJ0jRMObiTvAr4GvCRqvoFcAfwemANcAD4zGjTCd4+biokyaYkO5LsmHavJWkRm1JwJzmFQWh/uaq+DlBVz1bVkap6GfgCv5kOGQFWDb19JbB/7GdW1eaqWltVa49nAJK02ExlVUmALwJ7quqzQ/XlQ83eC+xq+1uBa5OcluQCYDXwyOx1WZIWt6VTaHM58BfAk0l2ttrHgOuSrGEwDfIM8CGAqtqd5F7gB8Bh4MZjrSiRJE3PpMsBT0gnXA4oSePMeDmgJOnkYnBLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZ6bysODTkzyS5PEku5N8otUvSPJwkqeTfCXJqa1+Wjve286fP7dDkKTFZSp33C8BV1bVW4A1wPok64BPAbdV1WrgeeCG1v4G4PmqegNwW2snSZolkwZ3DRxqh6e0rYArga+2+hbgmra/oR3Tzv9xkgkfeClJmr4pzXEnWZJkJ3AQ2Ab8GHihqg63JiPAira/AtgH0M6/CJw1m52WpMVsSsFdVUeqag2wErgMuGiiZu11orvrGltIsinJjiQ7ptpZSdI0V5VU1QvAQ8A6YFmSpe3USmB/2x8BVgG0868BnpvgszZX1dqqWjuzrkvS4jSVVSXnJFnW9l8BvBPYAzwIvK812wjc1/a3tmPa+QeqatwdtyRpZjJZpiZ5M4M/Ni5hEPT3VtU/JLkQuAc4E/g+8OdV9VKS04F/AS5hcKd9bVX9ZJJrGOySNEZVTbiwY9LgPhEMbkka72jB7S8nJakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1ZioPCz49ySNJHk+yO8knWv1LSX6aZGfb1rR6ktyeZG+SJ5JcOteDkKTFZOkU2rwEXFlVh5KcAvx3kv9s5/66qr46pv1VwOq2vRW4o71KkmbBpHfcNXCoHZ7StmM93HcDcFd733eBZUmWH39XJUkwxTnuJEuS7AQOAtuq6uF26pNtOuS2JKe12gpg39DbR1pNkjQLphTcVXWkqtYAK4HLkrwJuAX4A+APgTOBv23NJ3qc/Lg79CSbkuxIsmNGPZekRWpaq0qq6gXgIWB9VR1o0yEvAf8MXNaajQCrht62Etg/wWdtrqq1VbV2Rj2XpEVqKqtKzkmyrO2/Angn8MPReeskAa4BdrW3bAWub6tL1gEvVtWBOem9JC1CU1lVshzYkmQJg6C/t6ruT/JAknMYTI3sBP6qtf8P4GpgL/Ar4IOz321JWrxSdawFIieoE8n8d0KSTjJVNdHfDP3lpCT1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6szS+e5Acwh4ar47MUfOBv5nvjsxBxbquGDhjs1x9eX3jnbiZAnup6pq7Xx3Yi4k2bEQx7ZQxwULd2yOa+FwqkSSOmNwS1JnTpbg3jzfHZhDC3VsC3VcsHDH5rgWiFTVfPdBkjQNJ8sdtyRpiuY9uJOsT/JUkr1Jbp7v/kxXkjuTHEyya6h2ZpJtSZ5ur2e0epLc3sb6RJJL56/nx5ZkVZIHk+xJsjvJTa3e9diSnJ7kkSSPt3F9otUvSPJwG9dXkpza6qe1473t/Pnz2f/JJFmS5PtJ7m/HC2VczyR5MsnOJDtarevv4vGY1+BOsgT4R+Aq4GLguiQXz2efZuBLwPoxtZuB7VW1GtjejmEwztVt2wTccYL6OBOHgY9W1UXAOuDG9r9N72N7Cbiyqt4CrAHWJ1kHfAq4rY3reeCG1v4G4PmqegNwW2t3MrsJ2DN0vFDGBfCOqloztPSv9+/izFXVvG3A24BvDh3fAtwyn32a4TjOB3YNHT8FLG/7yxmsUwf4J+C6idqd7BtwH/CuhTQ24HeBx4C3MvgBx9JW//X3Evgm8La2v7S1y3z3/SjjWckgwK4E7geyEMbV+vgMcPaY2oL5Lk53m++pkhXAvqHjkVbr3XlVdQCgvZ7b6l2Ot/0z+hLgYRbA2Np0wk7gILAN+DHwQlUdbk2G+/7rcbXzLwJnndgeT9nngL8BXm7HZ7EwxgVQwLeSPJpkU6t1/12cqfn+5WQmqC3kZS7djTfJq4CvAR+pql8kEw1h0HSC2kk5tqo6AqxJsgz4BnDRRM3aaxfjSvInwMGqejTJFaPlCZp2Na4hl1fV/iTnAtuS/PAYbXsb27TN9x33CLBq6HglsH+e+jKbnk2yHKC9Hmz1rsab5BQGof3lqvp6Ky+IsQFU1QvAQwzm8JclGb2RGe77r8fVzr8GeO7E9nRKLgf+NMkzwD0Mpks+R//jAqCq9rfXgwz+Y3sZC+i7OF3zHdzfA1a3v3yfClwLbJ3nPs2GrcDGtr+RwfzwaP369lfvdcCLo//UO9lkcGv9RWBPVX126FTXY0tyTrvTJskrgHcy+GPeg8D7WrOx4xod7/uAB6pNnJ5MquqWqlpZVecz+P/RA1X1Z3Q+LoAkr0zy6tF94N3ALjr/Lh6X+Z5kB64GfsRgnvHv5rs/M+j/3cAB4P8Y/Jf+BgZzhduBp9vrma1tGKyi+THwJLB2vvt/jHG9ncE/L58Adrbt6t7HBrwZ+H4b1y7g71v9QuARYC/wb8BprX56O97bzl8432OYwhivAO5fKONqY3i8bbtHc6L37+LxbP5yUpI6M99TJZKkaTK4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqzP8DkHnZf7V3/zkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(prev_screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, state_Size, action_size, hidden_size):\n",
    "        super(ActorNet, self).__init__()\n",
    "        self.dense_layer_1 = nn.Linear(state_size, hidden_size)\n",
    "        self.dense_layer_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.clamp(x, -1.1, 1.1)\n",
    "        x = F.relu(self.dense_layer_1(x))\n",
    "        x = F.relu(self.dense_layer_2(x))\n",
    "        return F.softmax(self.out(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGAgent():\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate, discount):\n",
    "        self.action_size = action_size\n",
    "        self.actor_net = ActorNet(state_size, action_size, hidden_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.actor_net.parameters(), lr=learning_rate)\n",
    "        self.discount = discount\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            input_state = torch.FloatTensor(state).to(device)\n",
    "            action_probs = self.actor_net(input_state)\n",
    "            action_probs = action_probs.detach().cpu().numpy()\n",
    "            action = np.random.choice(np.arange(self.action_size), p=action_probs)\n",
    "        return action\n",
    "    \n",
    "    def train(self, state_list, action_list, reward_list):\n",
    "        trajectory_len = len(reward_list)\n",
    "        return_array = np.zeros((trajectory_len,))\n",
    "        g_return = 0.\n",
    "        for i in range(trajectory_len - 1, -1, -1):\n",
    "            g_return = reward_list[i] + self.discount * g_return\n",
    "            return_array[i] = g_return\n",
    "        \n",
    "        state_t = torch.FloatTensor(state_list).to(device)\n",
    "        action_t = torch.LongTensor(action_list).to(device).view(-1, 1)\n",
    "        return_t = torch.FloatTensor(return_array).to(device).view(-1, 1)\n",
    "        \n",
    "        selected_action_probs = self.actor_net(state_t).gather(1, action_t)\n",
    "        loss = torch.mean(-torch.log(selected_action_probs) * return_t)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.shape[0]\n",
    "seed = 31\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "episodes = 5000\n",
    "hidden_size = 256\n",
    "learning_rate = 0.0005\n",
    "discount = 0.99\n",
    "reward_scale = 0.01\n",
    "\n",
    "agent = PGAgent(state_size, action_size, hidden_size, learning_rate, discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Timestep: 58 Total reward: -112.2 Episode length: 58.0 Loss: -0.102\n",
      "Episode 10 Timestep: 943 Total reward: -192.9 Episode length: 88.5 Loss: -0.211\n",
      "Episode 20 Timestep: 1936 Total reward: -121.6 Episode length: 99.3 Loss: -0.055\n",
      "Episode 30 Timestep: 2909 Total reward: -198.3 Episode length: 97.3 Loss: -0.098\n",
      "Episode 40 Timestep: 4165 Total reward: -165.9 Episode length: 125.6 Loss: -0.137\n",
      "Episode 50 Timestep: 5294 Total reward: -217.4 Episode length: 112.9 Loss: -0.060\n",
      "Episode 60 Timestep: 6502 Total reward: -172.1 Episode length: 120.8 Loss: -0.087\n",
      "Episode 70 Timestep: 7493 Total reward: -158.8 Episode length: 99.1 Loss: -0.065\n",
      "Episode 80 Timestep: 8444 Total reward: -180.0 Episode length: 95.1 Loss: -0.126\n",
      "Episode 90 Timestep: 9467 Total reward: -159.7 Episode length: 102.3 Loss: -0.080\n",
      "Episode 100 Timestep: 10466 Total reward: -138.2 Episode length: 99.9 Loss: -0.064\n",
      "Episode 110 Timestep: 11406 Total reward: -158.3 Episode length: 94.0 Loss: -0.179\n",
      "Episode 120 Timestep: 12270 Total reward: -118.6 Episode length: 86.4 Loss: -0.077\n",
      "Episode 130 Timestep: 13213 Total reward: -132.8 Episode length: 94.3 Loss: -0.107\n",
      "Episode 140 Timestep: 14190 Total reward: -144.3 Episode length: 97.7 Loss: -0.109\n",
      "Episode 150 Timestep: 15180 Total reward: -149.0 Episode length: 99.0 Loss: -0.142\n",
      "Episode 160 Timestep: 16270 Total reward: -181.2 Episode length: 109.0 Loss: -0.067\n",
      "Episode 170 Timestep: 17327 Total reward: -195.6 Episode length: 105.7 Loss: -0.054\n",
      "Episode 180 Timestep: 18497 Total reward: -156.1 Episode length: 117.0 Loss: -0.089\n",
      "Episode 190 Timestep: 19554 Total reward: -151.9 Episode length: 105.7 Loss: -0.095\n",
      "Episode 200 Timestep: 20638 Total reward: -120.0 Episode length: 108.4 Loss: -0.046\n",
      "Episode 210 Timestep: 21685 Total reward: -144.2 Episode length: 104.7 Loss: -0.107\n",
      "Episode 220 Timestep: 22585 Total reward: -116.3 Episode length: 90.0 Loss: -0.080\n",
      "Episode 230 Timestep: 23518 Total reward: -155.4 Episode length: 93.3 Loss: -0.097\n",
      "Episode 240 Timestep: 24463 Total reward: -123.5 Episode length: 94.5 Loss: -0.048\n",
      "Episode 250 Timestep: 25368 Total reward: -166.3 Episode length: 90.5 Loss: -0.222\n",
      "Episode 260 Timestep: 26301 Total reward: -143.3 Episode length: 93.3 Loss: -0.124\n",
      "Episode 270 Timestep: 27369 Total reward: -135.5 Episode length: 106.8 Loss: -0.055\n",
      "Episode 280 Timestep: 28287 Total reward: -109.9 Episode length: 91.8 Loss: -0.154\n",
      "Episode 290 Timestep: 29436 Total reward: -134.0 Episode length: 114.9 Loss: -0.145\n",
      "Episode 300 Timestep: 30520 Total reward: -130.0 Episode length: 108.4 Loss: -0.069\n",
      "Episode 310 Timestep: 31508 Total reward: -104.0 Episode length: 98.8 Loss: -0.038\n",
      "Episode 320 Timestep: 32532 Total reward: -179.4 Episode length: 102.4 Loss: -0.168\n",
      "Episode 330 Timestep: 33766 Total reward: -117.1 Episode length: 123.4 Loss: -0.123\n",
      "Episode 340 Timestep: 35126 Total reward: -117.7 Episode length: 136.0 Loss: -0.078\n",
      "Episode 350 Timestep: 36489 Total reward: -115.5 Episode length: 136.3 Loss: -0.080\n",
      "Episode 360 Timestep: 37577 Total reward: -107.2 Episode length: 108.8 Loss: -0.062\n",
      "Episode 370 Timestep: 38823 Total reward: -179.0 Episode length: 124.6 Loss: -0.027\n",
      "Episode 380 Timestep: 40020 Total reward: -90.4 Episode length: 119.7 Loss: -0.038\n",
      "Episode 390 Timestep: 41196 Total reward: -162.0 Episode length: 117.6 Loss: -0.037\n",
      "Episode 400 Timestep: 42470 Total reward: -130.2 Episode length: 127.4 Loss: -0.008\n",
      "Episode 410 Timestep: 43683 Total reward: -103.7 Episode length: 121.3 Loss: -0.052\n",
      "Episode 420 Timestep: 44995 Total reward: -116.7 Episode length: 131.2 Loss: -0.047\n",
      "Episode 430 Timestep: 46435 Total reward: -91.7 Episode length: 144.0 Loss: -0.059\n",
      "Episode 440 Timestep: 47930 Total reward: -75.1 Episode length: 149.5 Loss: -0.027\n",
      "Episode 450 Timestep: 49675 Total reward: -145.0 Episode length: 174.5 Loss: -0.104\n",
      "Episode 460 Timestep: 51182 Total reward: -129.4 Episode length: 150.7 Loss: -0.007\n",
      "Episode 470 Timestep: 52774 Total reward: -102.7 Episode length: 159.2 Loss: -0.063\n",
      "Episode 480 Timestep: 54329 Total reward: -70.3 Episode length: 155.5 Loss: -0.109\n",
      "Episode 490 Timestep: 55768 Total reward: -96.9 Episode length: 143.9 Loss: -0.082\n",
      "Episode 500 Timestep: 57345 Total reward: -148.8 Episode length: 157.7 Loss: -0.094\n",
      "Episode 510 Timestep: 60848 Total reward: -137.6 Episode length: 350.3 Loss: -0.012\n",
      "Episode 520 Timestep: 62897 Total reward: -72.0 Episode length: 204.9 Loss: -0.010\n",
      "Episode 530 Timestep: 64537 Total reward: -45.6 Episode length: 164.0 Loss: -0.032\n",
      "Episode 540 Timestep: 67118 Total reward: -66.5 Episode length: 258.1 Loss: -0.026\n",
      "Episode 550 Timestep: 68691 Total reward: -35.5 Episode length: 157.3 Loss: -0.021\n",
      "Episode 560 Timestep: 71211 Total reward: -47.1 Episode length: 252.0 Loss: -0.042\n",
      "Episode 570 Timestep: 72882 Total reward: -73.8 Episode length: 167.1 Loss: -0.065\n",
      "Episode 580 Timestep: 74616 Total reward: -28.6 Episode length: 173.4 Loss: -0.022\n",
      "Episode 590 Timestep: 76008 Total reward: -44.4 Episode length: 139.2 Loss: -0.013\n",
      "Episode 600 Timestep: 77336 Total reward: -57.3 Episode length: 132.8 Loss: -0.067\n",
      "Episode 610 Timestep: 79314 Total reward: -58.1 Episode length: 197.8 Loss: -0.002\n",
      "Episode 620 Timestep: 80668 Total reward: -50.5 Episode length: 135.4 Loss: -0.045\n",
      "Episode 630 Timestep: 82441 Total reward: -110.9 Episode length: 177.3 Loss: -0.061\n",
      "Episode 640 Timestep: 86845 Total reward: -152.1 Episode length: 440.4 Loss: -0.050\n",
      "Episode 650 Timestep: 90229 Total reward: -173.5 Episode length: 338.4 Loss: -0.042\n",
      "Episode 660 Timestep: 94426 Total reward: -221.1 Episode length: 419.7 Loss: -0.045\n",
      "Episode 670 Timestep: 97063 Total reward: -56.0 Episode length: 263.7 Loss: -0.024\n",
      "Episode 680 Timestep: 98887 Total reward: -80.3 Episode length: 182.4 Loss: -0.006\n",
      "Episode 690 Timestep: 101229 Total reward: -8.7 Episode length: 234.2 Loss: -0.017\n",
      "Episode 700 Timestep: 102588 Total reward: -55.7 Episode length: 135.9 Loss: -0.052\n",
      "Episode 710 Timestep: 104745 Total reward: -26.2 Episode length: 215.7 Loss: 0.002\n",
      "Episode 720 Timestep: 106198 Total reward: -50.0 Episode length: 145.3 Loss: 0.005\n",
      "Episode 730 Timestep: 108707 Total reward: -48.6 Episode length: 250.9 Loss: -0.055\n",
      "Episode 740 Timestep: 110680 Total reward: -55.3 Episode length: 197.3 Loss: -0.002\n",
      "Episode 750 Timestep: 113292 Total reward: -87.4 Episode length: 261.2 Loss: -0.030\n",
      "Episode 760 Timestep: 115953 Total reward: -49.8 Episode length: 266.1 Loss: -0.026\n",
      "Episode 770 Timestep: 119232 Total reward: -92.7 Episode length: 327.9 Loss: -0.089\n",
      "Episode 780 Timestep: 122334 Total reward: -83.7 Episode length: 310.2 Loss: -0.037\n",
      "Episode 790 Timestep: 125203 Total reward: -42.9 Episode length: 286.9 Loss: -0.020\n",
      "Episode 800 Timestep: 127904 Total reward: -28.1 Episode length: 270.1 Loss: -0.054\n",
      "Episode 810 Timestep: 131243 Total reward: -18.7 Episode length: 333.9 Loss: 0.002\n",
      "Episode 820 Timestep: 136834 Total reward: -67.2 Episode length: 559.1 Loss: 0.013\n",
      "Episode 830 Timestep: 139516 Total reward: -50.2 Episode length: 268.2 Loss: 0.001\n",
      "Episode 840 Timestep: 142881 Total reward: -60.3 Episode length: 336.5 Loss: -0.039\n",
      "Episode 850 Timestep: 148258 Total reward: -69.6 Episode length: 537.7 Loss: 0.005\n",
      "Episode 860 Timestep: 154132 Total reward: -49.5 Episode length: 587.4 Loss: -0.030\n",
      "Episode 870 Timestep: 160019 Total reward: -91.8 Episode length: 588.7 Loss: -0.016\n",
      "Episode 880 Timestep: 166681 Total reward: -61.5 Episode length: 666.2 Loss: -0.014\n",
      "Episode 890 Timestep: 174054 Total reward: -81.1 Episode length: 737.3 Loss: -0.014\n",
      "Episode 900 Timestep: 178697 Total reward: -49.5 Episode length: 464.3 Loss: -0.015\n",
      "Episode 910 Timestep: 182778 Total reward: -31.6 Episode length: 408.1 Loss: -0.010\n",
      "Episode 920 Timestep: 189215 Total reward: -25.2 Episode length: 643.7 Loss: -0.011\n",
      "Episode 930 Timestep: 194048 Total reward: -36.7 Episode length: 483.3 Loss: -0.010\n",
      "Episode 940 Timestep: 198281 Total reward: -13.0 Episode length: 423.3 Loss: -0.005\n",
      "Episode 950 Timestep: 204118 Total reward: -63.7 Episode length: 583.7 Loss: -0.008\n",
      "Episode 960 Timestep: 211571 Total reward: -93.3 Episode length: 745.3 Loss: 0.000\n",
      "Episode 970 Timestep: 220311 Total reward: -61.0 Episode length: 874.0 Loss: -0.011\n",
      "Episode 980 Timestep: 228606 Total reward: -56.2 Episode length: 829.5 Loss: -0.041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 990 Timestep: 236477 Total reward: 2.6 Episode length: 787.1 Loss: -0.005\n",
      "Episode 1000 Timestep: 244062 Total reward: 4.6 Episode length: 758.5 Loss: 0.004\n",
      "Episode 1010 Timestep: 252541 Total reward: -1.5 Episode length: 847.9 Loss: -0.007\n",
      "Episode 1020 Timestep: 257666 Total reward: 6.0 Episode length: 512.5 Loss: -0.004\n",
      "Episode 1030 Timestep: 264017 Total reward: -3.4 Episode length: 635.1 Loss: -0.011\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-b60cf317ed41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtimesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    374\u001b[0m                                      color=(0.8, 0.8, 0))\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mgeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monetime_geoms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mgeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mdisable\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mglScalef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mglPopMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_translation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyglet/gl/lib.py\u001b[0m in \u001b[0;36merrcheck\u001b[0;34m(result, func, arguments)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mGLException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No GL context; create a Window first'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gl_begin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglGetError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgluErrorString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stats_rewards_list = []\n",
    "stats_every = 10\n",
    "total_reward = 0\n",
    "timesteps = 0\n",
    "episode_length = 0\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    stats_loss = 0.\n",
    "    \n",
    "    if len(stats_rewards_list) > stats_every and np.mean(stats_rewards_list[-stats_every:], axis=0)[1] > 190:\n",
    "        print(\"Stopping at episode {} with average rewards of {} in last {} episodes\".format(ep, np.mean(stats_rewards_list[-stats_every:], axis=0)[1], stats_every))\n",
    "        break\n",
    "    \n",
    "    state_list = []\n",
    "    action_list = []\n",
    "    reward_list = []\n",
    "    \n",
    "    while True:\n",
    "        timesteps += 1\n",
    "        env.render()\n",
    "        action = agent.select_action(state)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        episode_length += 1\n",
    "        state_list.append(state)\n",
    "        action_list.append(action)\n",
    "        reward_list.append(reward * reward_scale)\n",
    "        \n",
    "        if total_reward < -250:\n",
    "            done = 1\n",
    "            \n",
    "        if done:\n",
    "            stats_loss += agent.train(state_list, action_list, reward_list)\n",
    "            stats_rewards_list.append((ep, total_reward, episode_length))\n",
    "            total_reward = 0\n",
    "            episode_length = 0\n",
    "            if ep % stats_every == 0:\n",
    "                print('Episode {}'.format(ep),\n",
    "                     'Timestep: {}'.format(timesteps),\n",
    "                     'Total reward: {:.1f}'.format(np.mean(stats_rewards_list[-stats_every:], axis=0)[1]),\n",
    "                     'Episode length: {:.1f}'.format(np.mean(stats_rewards_list[-stats_every:], axis=0)[2]),\n",
    "                     'Loss: {:.3f}'.format(stats_loss/stats_every))\n",
    "            break\n",
    "            \n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
