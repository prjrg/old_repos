{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from gym import spaces\n",
    "import cv2\n",
    "cv2.ocl.setUseOpenCL(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNet(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size):\n",
    "        super(FeedForwardNeuralNet, self).__init__()\n",
    "        self.dl1 = nn.Linear(state_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dl2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm2d(hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.dl1(x)))\n",
    "        x = F.relu(self.bn2(self.dl2(x)))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size=1e6):\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        \n",
    "    def add(self, data):\n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = data\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else: self.storage.append(data)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        x, y, u, r, d = [], [], [], [], []\n",
    "        \n",
    "        for i in ind:\n",
    "            X, Y, U, R, D = self.storage[i]\n",
    "            x.append(np.array(X, copy=False))\n",
    "            y.append(np.array(Y, copy=False))\n",
    "            u.append(np.array(U, copy=False))\n",
    "            r.append(np.array(R, copy=False))\n",
    "            d.append(np.array(D, copy=False))\n",
    "            \n",
    "        return np.array(x), np.array(y), np.array(u).reshape(-1, 1), np.array(r).reshape(-1,1), np.array(d).reshape(-1, 1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1)\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "    \n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env, width=84, height=84, grayscale=True):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.grayscale = grayscale\n",
    "        shape = (1 if self.grayscale else 3, self.height, self.width)\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=shape, dtype=np.uint8)\n",
    "        \n",
    "    def observation(self, frame):\n",
    "        if self.grayscale:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        size = (self.width, self.height)\n",
    "        frame = cv2.resize(frame, size, interpolation=cv2.INTER_AREA)\n",
    "        if self.grayscale:\n",
    "            frame = np.expand_dims(frame, -1)\n",
    "        return frame.transpose((2, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('PongNoFrameskip-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, action_size, hidden_size):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.cv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.cv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.cv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.fc = nn.Linear(7 * 7 * 64, hidden_size)\n",
    "        self.bn4 = nn.BatchNorm1d(hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x / 255.0\n",
    "        x = F.relu(self.bn1(self.cv1(x)))\n",
    "        x = F.relu(self.bn2(self.cv2(x)))\n",
    "        x = F.relu(self.bn3(self.cv3(x)))\n",
    "        x = F.relu(self.bn4(self.fc(x.view(x.size(0), -1))))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, action_size, hidden_size, learning_rate):\n",
    "        self.action_size = action_size\n",
    "        self.train_net = DeepQNetwork(action_size, hidden_size).to(device)\n",
    "        self.target_net = DeepQNetwork(action_size, hidden_size).to(device)\n",
    "        self.target_net.load_state_dict(self.train_net.state_dict())\n",
    "        self.optimizer = optim.Adam(self.train_net.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def select_action(self, s, eps):\n",
    "        if np.random.rand() <= eps:\n",
    "            a = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                input_state = torch.FloatTensor(np.array(s)).unsqueeze(0).to(device)\n",
    "                a = self.train_net(input_state).max(1)[1]\n",
    "                a = int(a)\n",
    "        return a\n",
    "    \n",
    "    def train(self, replay_buffer, batch_size, discount):\n",
    "        x0, x1, a, r, d = replay_buffer.sample(batch_size)\n",
    "        state_batch = torch.FloatTensor(x0).to(device)\n",
    "        next_state_batch = torch.FloatTensor(x1).to(device)\n",
    "        action_batch = torch.LongTensor(a).to(device)\n",
    "        reward_batch = torch.FloatTensor(r).to(device)\n",
    "        done_batch = torch.FloatTensor(1. - d).to(device)\n",
    "        \n",
    "        train_q = self.train_net(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            target_net_q = reward_batch + done_batch * discount * \\\n",
    "            torch.max(self.target_net(next_state_batch).detach(), dim=1)[0].view(batch_size, -1)\n",
    "            \n",
    "        loss = F.smooth_l1_loss(train_q, target_net_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.train_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.detach().cpu().numpy()\n",
    "    \n",
    "    \n",
    "    def update_target_network(self, num_iter, update_every):\n",
    "        if num_iter % update_every == 0:\n",
    "            self.target_net.load_state_dict(self.train_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_size = 50000\n",
    "replay_buffer = ReplayBuffer(max_size=replay_size)\n",
    "update_target_every = 1000\n",
    "\n",
    "timesteps = 2000000\n",
    "hidden_size = 512\n",
    "learning_rate = 0.0001\n",
    "batch_size = 32\n",
    "start_training_after = 10001\n",
    "discount = 0.99\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay_steps = timesteps * .15\n",
    "epsilon_step = (epsilon_start - epsilon_min) / epsilon_decay_steps\n",
    "\n",
    "dqn_agent = DQNAgent(action_size, hidden_size, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_rewards_list = []\n",
    "stats_every = 10\n",
    "total_reward = 0\n",
    "episode = 1\n",
    "episode_length = 0\n",
    "stats_loss = 0.\n",
    "epsilon = epsilon_start\n",
    "state = env.reset()\n",
    "\n",
    "for ts in range(timesteps):\n",
    "    action = dqn_agent.select_action(state, epsilon)\n",
    "    epsilon -= epsilon_step\n",
    "    if epsilon < epsilon_min:\n",
    "        epsilon = epsilon_min\n",
    "        \n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    episode_length += 1\n",
    "    replay_buffer.add((state, next_state, action, reward, float(done)))\n",
    "    \n",
    "    if ts > start_training_after:\n",
    "        stats_loss = dqn_agent.train(replay_buffer, batch_size, discount)\n",
    "        dqn_agent.update_target_network(ts, update_target_every)\n",
    "        \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        stats_rewards_list.append((episode, total_reward, episode_length))\n",
    "        episode += 1\n",
    "        total_reward = 0\n",
    "        episode_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, action_size, hidden_size, learning_rate):\n",
    "        self.action_size = action_size\n",
    "        self.train_net = DeepQNetwork(action_size, hidden_size).to(device)\n",
    "        self.target_net = DeepQNetwork(action_size, hidden_size).to(device)\n",
    "        self.target_net.load_state_dict(self.train_net.state_dict())\n",
    "        self.optimizer = optim.Adam(self.train_net.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def select_action(self, s, eps):\n",
    "        if np.random.rand() <= eps:\n",
    "            a = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                input_state = torch.FloatTensor(np.array(s)).unsqueeze(0).to(device)\n",
    "                a = self.train_net(input_state).max(1)[1]\n",
    "                a = int(a)\n",
    "        return a\n",
    "    \n",
    "    def train(self, replay_buffer, batch_size, discount):\n",
    "        x0, x1, a, r, d = replay_buffer.sample(batch_size)\n",
    "        state_batch = torch.FloatTensor(x0).to(device)\n",
    "        next_state_batch = torch.FloatTensor(x1).to(device)\n",
    "        action_batch = torch.LongTensor(a).to(device)\n",
    "        reward_batch = torch.FloatTensor(r).to(device)\n",
    "        done_batch = torch.FloatTensor(1. - d).to(device)\n",
    "        \n",
    "        train_q = self.train_net(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            train_argmax = self.train_net(next_state_batch).max(1)[1].view(batch_size, 1)\n",
    "            target_net_q = reward_batch + done_batch * discount * \\\n",
    "                self.target_net(next_state_batch).gather(1, train_argmax)\n",
    "            \n",
    "        loss = F.smooth_l1_loss(train_q, target_net_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.train_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.detach().cpu().numpy()\n",
    "    \n",
    "    \n",
    "    def update_target_network(self, num_iter, update_every):\n",
    "        if num_iter % update_every == 0:\n",
    "            self.target_net.load_state_dict(self.train_net.state_dict())\n",
    "            \n",
    "    def update_target_network_soft(self, num_iter, update_every, update_tau=0.001):\n",
    "        if num_iter % update_every == 0:\n",
    "            for target_var, var in zip(self.target_net.parameters(), self.train_net.parameters()):\n",
    "                target_var.data.copy_((1. - update_tau) * target_var + update_tau * var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDeepQNetwork(nn.Module):\n",
    "    def __init__(self, action_size, hidden_size):\n",
    "        super(DuelingDeepQNetwork, self).__init__()\n",
    "        self.cv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.cv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.cv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.fc = nn.Linear(7 * 7 * 64, hidden_size)\n",
    "        self.bn4 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dueling_value = nn.Linear(hidden_size, 1)\n",
    "        self.dueling_action = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x / 255.0\n",
    "        x = F.relu(self.bn1(self.cv1(x)))\n",
    "        x = F.relu(self.bn2(self.cv2(x)))\n",
    "        x = F.relu(self.bn3(self.cv3(x)))\n",
    "        x = F.relu(self.bn4(self.fc(x.view(x.size(0), -1))))\n",
    "        x = self.dueling_action(x) - self.duelin_action(x).mean(dim=1, keepdim=True) + self.dueling_value(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
