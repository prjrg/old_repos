{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_q_value_estimate(env, episodes=1000, discount_factor=1.0, epsilon=0.1):\n",
    "    state_size = env.nS\n",
    "    action_size = env.nA\n",
    "    max_timesteps = 100\n",
    "    timesteps = 0\n",
    "    q_value_array = np.zeros((state_size, action_size))\n",
    "    q_return_array = np.zeros((state_size, action_size, 2))\n",
    "    trajectory_list = []\n",
    "    \n",
    "    current_state = env.reset()\n",
    "    \n",
    "    current_episode = 0\n",
    "    \n",
    "    while current_episode < episodes:\n",
    "        if np.random.rand() < epsilon:\n",
    "            eg_action = env.action_space.sample()\n",
    "        else:\n",
    "            argmax_index = np.argmax(q_value_array[current_state])\n",
    "            argmax_value = q_value_array[current_state][argmax_index]\n",
    "            greedy_indices = np.argwhere(q_value_array[current_State] == argmax_value).reshape(-1)\n",
    "            eg_action = np.random.choice(greedy_indices)\n",
    "        next_state, rew, done, info = env.step(eg_action)\n",
    "        trajectory_list.append((current_state, rew, eg_action))\n",
    "        \n",
    "        timesteps += 1\n",
    "        if timesteps > max_timesteps:\n",
    "            done = 1\n",
    "            \n",
    "        if done:\n",
    "            q_value_array, q_return_array = monte_carlo_first_visit_update(q_value_array, q_return_array, trajectory_list)\n",
    "            trajectory_list = []\n",
    "            timesteps = 0\n",
    "            current_state = env.reset()\n",
    "            current_episode += 1\n",
    "        else:\n",
    "            current_state = next_state\n",
    "            \n",
    "        return q_value_array, q_return_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_first_visit_update(q_values, q_returns, traj, discount=1.):\n",
    "    g_return = 0\n",
    "    first_visit_dict = {}\n",
    "    for t in range(len(traj)-1, -1, -1):\n",
    "        state, reward, action = traj[t]\n",
    "        g_return = discount * g_return + reward\n",
    "        if (state, action) not in first_visit_dict:\n",
    "            first_visit_dict[(state, action)] = 1\n",
    "            q_returns[state][action][1] += 1\n",
    "            q_returns[state][action][0] = (q_returns[state][action][0]*(q_returns[state][action][1]-1) / q_returns[state][action][0])\n",
    "            q_values[state][action] = q_returns[state][action][0]\n",
    "    return q_values, q_returns \n",
    "                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
