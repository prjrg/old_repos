{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size, action_max):\n",
    "        super(ActorNet, self).__init__()\n",
    "        self.dense_layer_1 = nn.Linear(state_size, hidden_size)\n",
    "        self.dense_layer_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, action_size)\n",
    "        self.action_max = action_max\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.clamp(x, -1.1, 1.1)\n",
    "        x = F.relu(self.dense_layer_1(x))\n",
    "        x = F.relu(self.dense_layer_2(x))\n",
    "        return torch.tanh(self.out(x)) * self.action_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNet(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size,):\n",
    "        super(CriticNet, self).__init__()\n",
    "        self.dense_layer_1 = nn.Linear(state_size + action_size, hidden_size)\n",
    "        self.dense_layer_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x, a):\n",
    "        x = torch.clamp(x, -1.1, 1.1)\n",
    "        x = F.relu(self.dense_layer_1(torch.cat((x,a), dim=1)))\n",
    "        x = F.relu(self.dense_layer_2(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size=1e6):\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        \n",
    "    def add(self, data):\n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = data\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(data)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        x, y, u, r, d = [], [], [], [], []\n",
    "        \n",
    "        for i in ind:\n",
    "            X, Y, U, R, D = self.storage[i]\n",
    "            x.append(np.array(X, copy=False))\n",
    "            y.append(np.array(Y, copy=False))\n",
    "            u.append(np.array(U, copy=False))\n",
    "            r.append(np.array(R, copy=False))\n",
    "            d.append(np.array(D, copy=False))\n",
    "            \n",
    "        return np.array(x), np.array(y), np.array(u), np.array(r).reshape(-1,1), np.array(d).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent():\n",
    "    def __init__(self, state_size, action_size, hidden_size, actor_lr, critic_lr, discount,\n",
    "                 min_action, max_action, exploration_noise):\n",
    "        self.action_size = action_size\n",
    "        self.actor = ActorNet(state_size, action_size, hidden_size, max_action).to(device)\n",
    "        self.actor_target = ActorNet(state_size, action_size, hidden_size, max_action).to(device)\n",
    "        self.critic = CriticNet(state_size, action_size, hidden_size).to(device)\n",
    "        self.critic_target = CriticNet(state_size, action_size, hidden_size).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        \n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.discount = discount\n",
    "        self.min_action = min_action\n",
    "        self.max_action = max_action\n",
    "        self.exploration_noise = exploration_noise\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        #get action probs then randomly sample from the probabilities\n",
    "        with torch.no_grad():\n",
    "            input_state = torch.FloatTensor(state).to(device)\n",
    "            action = self.actor(input_state)\n",
    "            #detach and turn to numpy to use with np.random.choice()\n",
    "            action = action.detach().cpu().numpy()\n",
    "            #in DDPG add noise for exploration\n",
    "            action = (action + np.random.normal(0., self.exploration_noise, \n",
    "                       size=self.action_size)).clip(self.min_action, self.max_action)   \n",
    "        return action\n",
    "\n",
    "    \n",
    "    def train(self, replay_buffer, batch_size):\n",
    "        # sample a batch from the replay buffer\n",
    "        x0, x1, a, r, d = replay_buffer.sample(batch_size)\n",
    "        # turn batches into tensors and use GPU if available\n",
    "        state_batch = torch.FloatTensor(x0).to(device)\n",
    "        next_state_batch = torch.FloatTensor(x1).to(device)\n",
    "        action_batch = torch.FloatTensor(a).to(device)\n",
    "        reward_batch = torch.FloatTensor(r).to(device)\n",
    "        flipped_done_batch = torch.FloatTensor(d).to(device) #already flipped done when adding to replay buffer\n",
    "\n",
    "        # get target net target values\n",
    "        with torch.no_grad():\n",
    "            target_action = self.actor_target(next_state_batch).view(batch_size,-1)\n",
    "            target_v = reward_batch + flipped_done_batch*self.discount*self.critic_target(next_state_batch, \n",
    "                                                                           target_action).view(batch_size,-1)\n",
    "        # get train net values for updating the critic network    \n",
    "        critic_v = self.critic(state_batch, action_batch).view(batch_size,-1)\n",
    "        \n",
    "        # train critic\n",
    "        critic_loss = F.smooth_l1_loss(critic_v, target_v)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step() \n",
    "        \n",
    "        # train actor\n",
    "        train_action = self.actor(state_batch)\n",
    "        actor_loss = -torch.mean(self.critic(state_batch,train_action))\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step() \n",
    "        \n",
    "        return actor_loss.detach().cpu().numpy(), critic_loss.detach().cpu().numpy()\n",
    "        \n",
    "        \n",
    "    def update_target_network_soft(self, num_iter, update_every, update_tau=0.001):\n",
    "        # soft target network update: update target networks with mixture of train and target\n",
    "        if num_iter % update_every == 0:\n",
    "            for target_var, var in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "                target_var.data.copy_((1.-update_tau) * target_var.data + (update_tau) * var.data)\n",
    "            for target_var, var in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "                target_var.data.copy_((1.-update_tau) * target_var.data + (update_tau) * var.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize env and set up hyperparameters\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "action_size = env.action_space.shape[0]\n",
    "state_size = env.observation_space.shape[0]\n",
    "min_action = env.action_space.low[0]\n",
    "max_action = env.action_space.high[0]\n",
    "\n",
    "# set seed\n",
    "seed = 31\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "# create replay buffer\n",
    "replay_size = 50000 # size of replay buffer\n",
    "replay_buffer = ReplayBuffer(max_size=replay_size)    \n",
    "\n",
    "# target update hyperparameters\n",
    "start_training_after = 10001 # start training NN after this many timesteps\n",
    "update_target_every = 1 # update target network every this steps\n",
    "tau = 0.001\n",
    "\n",
    "episodes = 5000    \n",
    "discount = 0.99\n",
    "batch_size = 32\n",
    "exploration_noise = 0.1\n",
    "hidden_size = 64\n",
    "actor_lr = 0.0005\n",
    "critic_lr = 0.0005\n",
    "reward_scale = 0.01\n",
    "\n",
    "# create DDPG Agent\n",
    "agent = DDPGAgent(state_size=state_size, action_size=action_size, hidden_size=hidden_size, \n",
    "                  actor_lr=actor_lr, critic_lr=critic_lr, discount=discount, min_action=min_action,\n",
    "                  max_action=max_action, exploration_noise=exploration_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 90 Timestep: 10340 Total reward: -214.2 Episode length: 113.7 Actor Loss: -0.0589 Critic Loss: 0.0049\n",
      "Episode: 100 Timestep: 11606 Total reward: -282.5 Episode length: 126.6 Actor Loss: -0.0561 Critic Loss: 0.0034\n",
      "Episode: 110 Timestep: 13381 Total reward: -255.8 Episode length: 177.5 Actor Loss: -0.0597 Critic Loss: 0.0025\n",
      "Episode: 120 Timestep: 14690 Total reward: -252.2 Episode length: 130.9 Actor Loss: -0.0643 Critic Loss: 0.0021\n",
      "Episode: 130 Timestep: 16211 Total reward: -247.7 Episode length: 152.1 Actor Loss: -0.0583 Critic Loss: 0.0018\n",
      "Episode: 140 Timestep: 18068 Total reward: -240.0 Episode length: 185.7 Actor Loss: -0.0511 Critic Loss: 0.0017\n",
      "Episode: 150 Timestep: 19861 Total reward: -262.4 Episode length: 179.3 Actor Loss: -0.0378 Critic Loss: 0.0016\n",
      "Episode: 160 Timestep: 22144 Total reward: -214.6 Episode length: 228.3 Actor Loss: -0.0261 Critic Loss: 0.0014\n",
      "Episode: 170 Timestep: 24604 Total reward: -196.8 Episode length: 246.0 Actor Loss: -0.0293 Critic Loss: 0.0014\n",
      "Episode: 180 Timestep: 26885 Total reward: -218.3 Episode length: 228.1 Actor Loss: -0.0272 Critic Loss: 0.0012\n",
      "Episode: 190 Timestep: 29107 Total reward: -249.1 Episode length: 222.2 Actor Loss: -0.0403 Critic Loss: 0.0014\n",
      "Episode: 200 Timestep: 31962 Total reward: -264.4 Episode length: 285.5 Actor Loss: -0.0626 Critic Loss: 0.0012\n",
      "Episode: 210 Timestep: 34356 Total reward: -249.1 Episode length: 239.4 Actor Loss: -0.0809 Critic Loss: 0.0012\n",
      "Episode: 220 Timestep: 35965 Total reward: -226.8 Episode length: 160.9 Actor Loss: -0.0906 Critic Loss: 0.0012\n",
      "Episode: 230 Timestep: 38006 Total reward: -205.2 Episode length: 204.1 Actor Loss: -0.1009 Critic Loss: 0.0012\n",
      "Episode: 240 Timestep: 40044 Total reward: -192.8 Episode length: 203.8 Actor Loss: -0.1118 Critic Loss: 0.0012\n",
      "Episode: 250 Timestep: 41930 Total reward: -198.0 Episode length: 188.6 Actor Loss: -0.1164 Critic Loss: 0.0013\n",
      "Episode: 260 Timestep: 44074 Total reward: -188.4 Episode length: 214.4 Actor Loss: -0.1222 Critic Loss: 0.0012\n",
      "Episode: 270 Timestep: 46362 Total reward: -186.5 Episode length: 228.8 Actor Loss: -0.1323 Critic Loss: 0.0012\n",
      "Episode: 280 Timestep: 47905 Total reward: -212.8 Episode length: 154.3 Actor Loss: -0.1431 Critic Loss: 0.0011\n",
      "Episode: 290 Timestep: 50033 Total reward: -207.4 Episode length: 212.8 Actor Loss: -0.1445 Critic Loss: 0.0012\n",
      "Episode: 300 Timestep: 53860 Total reward: -245.5 Episode length: 382.7 Actor Loss: -0.1542 Critic Loss: 0.0012\n",
      "Episode: 310 Timestep: 60565 Total reward: -187.6 Episode length: 670.5 Actor Loss: -0.1920 Critic Loss: 0.0011\n",
      "Episode: 320 Timestep: 65096 Total reward: -165.9 Episode length: 453.1 Actor Loss: -0.2325 Critic Loss: 0.0011\n",
      "Episode: 330 Timestep: 71526 Total reward: -194.5 Episode length: 643.0 Actor Loss: -0.2697 Critic Loss: 0.0010\n",
      "Episode: 340 Timestep: 76023 Total reward: -218.9 Episode length: 449.7 Actor Loss: -0.3075 Critic Loss: 0.0010\n",
      "Episode: 350 Timestep: 81928 Total reward: -162.9 Episode length: 590.5 Actor Loss: -0.3509 Critic Loss: 0.0009\n",
      "Episode: 360 Timestep: 90908 Total reward: -147.2 Episode length: 898.0 Actor Loss: -0.4559 Critic Loss: 0.0011\n",
      "Episode: 370 Timestep: 98565 Total reward: -159.2 Episode length: 765.7 Actor Loss: -0.5121 Critic Loss: 0.0009\n",
      "Episode: 380 Timestep: 104122 Total reward: -65.6 Episode length: 555.7 Actor Loss: -0.5634 Critic Loss: 0.0009\n",
      "Episode: 390 Timestep: 108939 Total reward: -19.7 Episode length: 481.7 Actor Loss: -0.6144 Critic Loss: 0.0009\n",
      "Episode: 400 Timestep: 114403 Total reward: -11.3 Episode length: 546.4 Actor Loss: -0.6441 Critic Loss: 0.0009\n",
      "Episode: 410 Timestep: 123270 Total reward: -54.4 Episode length: 886.7 Actor Loss: -0.6922 Critic Loss: 0.0011\n",
      "Episode: 420 Timestep: 130316 Total reward: -40.9 Episode length: 704.6 Actor Loss: -0.7676 Critic Loss: 0.0011\n",
      "Episode: 430 Timestep: 134428 Total reward: 52.3 Episode length: 411.2 Actor Loss: -0.8089 Critic Loss: 0.0011\n",
      "Stopping at episode 439 with average rewards of 206.92640690336611 in last 10 episodes\n"
     ]
    }
   ],
   "source": [
    "stats_rewards_list = [] # store stats for plotting in this\n",
    "stats_every = 10 # print stats every this many episodes\n",
    "total_reward = 0\n",
    "timesteps = 0\n",
    "episode_length = 0\n",
    "stats_actor_loss, stats_critic_loss = [], []\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "\n",
    "    # stopping condition for training if agent reaches the amount of reward\n",
    "    if len(stats_rewards_list) > stats_every and np.mean(stats_rewards_list[-stats_every:],axis=0)[1] > 190:\n",
    "        print(\"Stopping at episode {} with average rewards of {} in last {} episodes\".\n",
    "            format(ep, np.mean(stats_rewards_list[-stats_every:],axis=0)[1], stats_every))\n",
    "        break  \n",
    "\n",
    "    # train in each episode until episode is done\n",
    "    while True:\n",
    "        timesteps += 1\n",
    "        #env.render()\n",
    "        # select an action from the agent's policy\n",
    "        action = agent.select_action(state)\n",
    "        \n",
    "        # enter action into the env\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        episode_length += 1\n",
    "        # end episode early\n",
    "        if total_reward < -250:\n",
    "            done = 1\n",
    "        \n",
    "        # add experience to replay buffer\n",
    "        replay_buffer.add((state, next_state, action, reward*reward_scale, 1-float(done)))\n",
    "        \n",
    "        # train the agent\n",
    "        if timesteps >= start_training_after:\n",
    "            actor_loss, critic_loss = agent.train(replay_buffer, batch_size)\n",
    "            stats_actor_loss.append(actor_loss) \n",
    "            stats_critic_loss.append(critic_loss) \n",
    "            agent.update_target_network_soft(timesteps, update_target_every)\n",
    "            \n",
    "        if done:\n",
    "            stats_rewards_list.append((ep, total_reward, episode_length))\n",
    "            total_reward = 0\n",
    "            episode_length = 0  \n",
    "            if timesteps >= start_training_after and ep % stats_every == 0:\n",
    "                print('Episode: {}'.format(ep),\n",
    "                    'Timestep: {}'.format(timesteps),\n",
    "                    'Total reward: {:.1f}'.format(np.mean(stats_rewards_list[-stats_every:],axis=0)[1]),\n",
    "                    'Episode length: {:.1f}'.format(np.mean(stats_rewards_list[-stats_every:],axis=0)[2]),\n",
    "                    'Actor Loss: {:.4f}'.format(np.mean(stats_actor_loss)), \n",
    "                    'Critic Loss: {:.4f}'.format(np.mean(stats_critic_loss)))\n",
    "                stats_actor_loss, stats_critic_loss = [], []\n",
    "            break\n",
    "        \n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
