{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project used Double Deep Q Learning with Experience Replay and Dueling Q Networks. \n",
    "It was mainly based on the Deep Q Networks codebase from the classroom practice exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `Navigation.ipynb` includes the training of the Double Dueling Q Agent. <br/>\n",
    "The file `ddqn_agent.py` implements the Double Q Learning Agent. <br/>\n",
    "The file `dueling_model.py` implements the Dueling neural network model <br/>\n",
    "The file `training.py` implements the method to actually train the agent. <br/>\n",
    "\n",
    "Most of the code is a copy from the codebase of the classroom assignment of Deep Q Networks with minor changes to implement Double Q Learning and Dueling Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was a Deep Dueling Q Network with 4 fully connected layers, in contrast to the 3 layered model from the Deep Q Network assignment in the classroom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent was the same as the Deep Q Networks with the small change of the Q_targets looking into the maximal actions from the local model values. The minor line additions were, in the file `ddqn_agent.py`:<br/>\n",
    "**expected_actions = torch.argmax(self.qnetwork_local(next_states).detach(), 1).unsqueeze(1)** <br/>\n",
    "**Q_targets_next = self.qnetwork_target(next_states).gather(1, expected_actions)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters for the agent were: <br/> \n",
    "1. The number of episodes\n",
    "2. The epsilon end, epsilon start and epsilon step for the epsilon-greedy policy choice \n",
    "3. The Buffer size for the replay buffer in experience replay \n",
    "4. Gamma for the iteration update\n",
    "5. Tau for the soft copy update\n",
    "6. The learning rate for the stochastic gradient descent algorithm\n",
    "7. The number of steps needed to update the target model (the target neural network)\n",
    "\n",
    "Their values are presented in the header of the `ddqn_agent.py` file, and the values are respectively: <br/>\n",
    "NUM_EPISODES = 1800 <br/>\n",
    "BUFFER_SIZE = int(1e5)  <br/>\n",
    "BATCH_SIZE = 64       <br/> \n",
    "GAMMA = 0.99        <br/>\n",
    "TAU = 1e-3           <br/>\n",
    "LR = 5e-4           <br/> \n",
    "UPDATE_EVERY = 4  <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neural network applied was a Dueling Neural Network with 4 fully connected layers and its code can be found in the `dueling_model.py` file. <br/>\n",
    "The architecture can be described as: <br/>\n",
    "64 units Hidden Layer 1 -> 64 units Hidden Layer 2 -> 64 units hidden layer 3 -> 4th layer added to the value function for the Dueling Q Network\n",
    "\n",
    "In summary, all hidden layers have 64 units and the last layer, the fourth - which then represents the advantage function -, is being added to the value function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewards plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Rewards over 100 consecutive episodes](./solved.png)\n",
    "\n",
    "In the graphic above you can see the rewards of the trained environment running over 100 consecutive episodes. Mainly every episode scores above 13.0 and it shows that the rewards are above 13.0 over the 100 episodes, which means the environment is solved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment was considered solved after about 500 episodes (iterations), which means it achieved an average score of more than 13.0 over the last 100 episodes after the 500th episode. \n",
    "\n",
    "The training algorithm stopped only after having achieved a score higher than 16.5 so it would have a margin above 13.0 to score higher than the requirement to be considered solved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Ideas To Improve Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Improve codebase, clean code, redundancies and comments.\n",
    "2. Organize the code better with more functions and files.\n",
    "3. Implement Prioritized Experience Replay.\n",
    "\n",
    "Other activities that might be helpful:\n",
    "4. Do a Youtube video showing the agent in action.\n",
    "5. Work on a Medium article to explain how my project was achieved and what it is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
